{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: (878049, 9), output dimensions: (878049L,)\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import gzip\n",
    "import csv\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "def dating(x):\n",
    "    date, time = x.split(' ')\n",
    "    y, m, d = map(int, date.split('-'))\n",
    "    h = int(time.split(':')[0])\n",
    "    return [y, m, d, h]\n",
    "\n",
    "\n",
    "def create_submission(preds):\n",
    "    with gzip.open('submission{0}.gz'.format(time()), 'wt') as outf:\n",
    "        fo = csv.writer(outf, lineterminator='\\n')\n",
    "        fo.writerow('Id,ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS'.split(','))\n",
    "        for i, pred in enumerate(preds):\n",
    "            fo.writerow([i] + list(pred))\n",
    "\n",
    "def load_data(trainpath='input/train.csv', testpath = 'input/test.csv'):\n",
    "    from sklearn import preprocessing\n",
    "    def load_(filepath):\n",
    "        msg_path = filepath.split('.')[0]+'.msg'\n",
    "        if path.isfile(msg_path):\n",
    "            df_data = pd.read_msgpack(msg_path)\n",
    "        else:\n",
    "            df_data = pd.read_csv(filepath)\n",
    "            dates = pd.DataFrame([dating(date) for date in df_data['Dates']], columns=['Year', 'Month', 'Day', 'Hour'])\n",
    "            df_data.drop(['Dates'],  axis=1, inplace=True)\n",
    "            df_data = pd.concat((df_data, dates), axis=1)\n",
    "            df_data.replace({'DayOfWeek':{'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}}, inplace=True)\n",
    "            \n",
    "            vec = preprocessing.LabelEncoder()\n",
    "            df_data['Address'] = vec.fit_transform(df_data['Address'])\n",
    "            df_data['PdDistrict'] = vec.fit_transform(df_data['PdDistrict'])\n",
    "            df_data.to_msgpack(msg_path)\n",
    "        return df_data\n",
    "    \n",
    "    df_train = load_(trainpath).apply(np.random.permutation)\n",
    "    X_train = df_train.drop(['Category', 'Resolution', 'Descript'], axis=1)#preprocess_data(\n",
    "    Y_train = preprocessing.LabelEncoder().fit_transform(df_train['Category'])\n",
    "    \n",
    "    X_test = load_(testpath).drop(['Id'],  axis=1) # preprocess_data(\n",
    "    return X_train, Y_train, X_test\n",
    "\n",
    "X_train, Y_train, X_test = load_data()\n",
    "\n",
    "input_dim = X_train.shape\n",
    "output_dim = Y_train.shape\n",
    "print('Input dimensions: {}, output dimensions: {}'.format(input_dim, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-58400f6b8f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def build_model(input_dim, output_dim, hn=32, dp=np.float32(0.5), layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hn, input_dim = input_dim))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(hn))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization(hn))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(output_dim))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCHES = 128\n",
    "HN = 64\n",
    "RUN_FOLDS = False\n",
    "nb_folds = 4\n",
    "kfolds = KFold(len(y), nb_folds)\n",
    "av_ll = 0.\n",
    "f = 0\n",
    "if RUN_FOLDS:\n",
    "    for train, valid in kfolds:\n",
    "        print('---' * 20)\n",
    "        print('Fold', f)\n",
    "        print('---' * 20)\n",
    "        f += 1\n",
    "        X_train = X[train]\n",
    "        X_valid = X[valid]\n",
    "        Y_train = Y[train]\n",
    "        Y_valid = Y[valid]\n",
    "        y_valid = y[valid]\n",
    "\n",
    "        print(\"Building model...\")\n",
    "        model = build_model(input_dim, output_dim, HN)\n",
    "\n",
    "        print(\"Training model...\")\n",
    "\n",
    "        model.fit(X_train, Y_train, nb_epoch=EPOCHS, batch_size=BATCHES, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "        valid_preds = model.predict_proba(X_valid)\n",
    "        ll = metrics.log_loss(y_valid, valid_preds)\n",
    "        print(\"LL:\", ll)\n",
    "        av_ll += ll\n",
    "    print('Average LL:', av_ll / nb_folds)\n",
    "    \n",
    "print(\"Generating submission...\")\n",
    "\n",
    "model = build_model(input_dim, output_dim, HN)\n",
    "model.fit(X, Y, nb_epoch=EPOCHS, batch_size=BATCHES, verbose=0)\n",
    "\n",
    "print('Predicting over testing data...')\n",
    "preds = model.predict_proba(X_test, verbose=0)\n",
    "\n",
    "create_subsmission(preds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try:\n",
    "\n",
    "k nearest neighbor\n",
    "random forest\n",
    "xgboost\n",
    "mlp\n",
    "Gaussian processes GPFlow\n",
    "Kernel density estimation\n",
    "\n",
    "\n",
    "Histograms or scatter-plots of different dimensions.\n",
    "PCA.\n",
    "Non-linear dimensionality reduction - Tisney, isomap, lle.\n",
    "Supervised - least squares, SVM, LR, for discrete data - desicion trees, KNN.\n",
    "\n",
    "\n",
    "try discritizing probabilities of very probable categories\n",
    "reduce number of features, since GP don't like high dimensionality\n",
    "\n",
    "remove address, stop scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=Y_test)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 6\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# get prediction\n",
    "pred = bst.predict( xg_test );\n",
    "\n",
    "print ('predicting, classification error=%f' % (sum( int(pred[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))\n",
    "\n",
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "yprob = bst.predict( xg_test ).reshape( test_Y.shape[0], 6 )\n",
    "ylabel = np.argmax(yprob, axis=1)\n",
    "\n",
    "print ('predicting, classification error=%f' % (sum( int(ylabel[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\forest.py:403: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "clf = RandomForestClassifier(warm_start=True, oob_score=True, max_features=\"sqrt\")\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(oob_score=True, bootstrap=True, max_depth=10, warm_start=True, n_estimators=10000, max_features=4, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=32)]: Done 736 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=32)]: Done 1186 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=32)]: Done 1736 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=32)]: Done 2386 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=32)]: Done 3136 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=32)]: Done 3986 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=32)]: Done 4936 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=32)]: Done 5986 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=32)]: Done 7136 tasks      | elapsed:   41.6s\n"
     ]
    }
   ],
   "source": [
    "# clf.set_params(n_jobs=1)\n",
    "# p = clf.predict_proba(X_test)\n",
    "clf.set_params(n_jobs=-1)\n",
    "x_test = np.array(X_test)\n",
    "ans = []\n",
    "for i in range(0, x_test.shape[0], 50000):\n",
    "    p = clf.predict_proba(x_test[i:i+50000,:])\n",
    "    ans.extend(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_submission(preds):\n",
    "    with gzip.open('submission{0}.gz'.format(time()), 'wt') as outf:\n",
    "        fo = csv.writer(outf, lineterminator='\\n')\n",
    "        fo.writerow('Id,ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS'.split(','))\n",
    "        for i, pred in enumerate(preds):\n",
    "            fo.writerow([i] + list(pred))\n",
    "\n",
    "create_submission(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-03315fc34079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878049, 39)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = Y_train.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878049,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "           max_depth=10, max_features=4, max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=1,\n",
       "           oob_score=True, random_state=None, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 10,\n",
       " 'max_features': 4,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10000,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': True,\n",
       " 'random_state': None,\n",
       " 'verbose': 1,\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19918478353713745"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 1 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[1 0 0 ..., 0 0 1]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 1 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 1 0 ..., 0 0 0]\n",
      "[0 0 0 ..., 0 0 0]\n",
      "[0 0 1 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import gaussian_process\n",
    "\n",
    "categories = set(Y_train)\n",
    "predictions = dict()\n",
    "\n",
    "for cat in categories:\n",
    "    selector = np.array((Y_train == cat), dtype = int)\n",
    "    x = X_train[selector]\n",
    "    y = Y_train[selector]\n",
    "    \n",
    "    gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)\n",
    "    gp.fit(x, y)\n",
    "\n",
    "    Y_pred, sigma2_pred = gp.predict(X_test, eval_MSE=True)\n",
    "    predictions[cat] = Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.965660650622816"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import gaussian_process\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import cross_validation\n",
    "\n",
    "n = 750\n",
    "x_train = X_train[:n]\n",
    "y_hot = np_utils.to_categorical(Y_train)[:n]\n",
    "\n",
    "x_train, x_test, y_hot, y_test = cross_validation.train_test_split(x_train, y_hot, test_size=0.05, random_state=0)\n",
    "\n",
    "gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)\n",
    "gp.fit(x_train, y_hot)\n",
    "y_pred, sigma2_pred = gp.predict(x_test, eval_MSE=True)\n",
    "log_loss(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
